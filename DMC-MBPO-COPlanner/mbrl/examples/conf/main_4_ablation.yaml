algorithm:
  # @package _group_
  name: "mbpo"

  normalize: true
  normalize_double_precision: true
  target_is_delta: true
  learned_rewards: true
  freq_train_model: ${overrides.freq_train_model}
  real_data_ratio: 0.5

  sac_samples_action: true
  initial_exploration_steps: 5000
  random_initial_explore: false
  num_eval_episodes: 1

  # --------------------------------------------
  #          MPC Agent configuration
  # --------------------------------------------
  ntraj: 5
  planning_horizon: 5
  reward_gamma: 0.97
  uncertainty_gamma: 0.97
  conservative_rate: 2
  optimistic_rate: 1


  # --------------------------------------------
  #          SAC Agent configuration
  # --------------------------------------------
  agent:
    _target_: mbrl.third_party.pytorch_sac_pranz24.sac.SAC
    num_inputs: 24
    action_space:
      _target_: gym.env.Box
      low: ???
      high: ???
      shape: ???
    args:
      gamma: ${overrides.sac_gamma}
      tau: ${overrides.sac_tau}
      alpha: ${overrides.sac_alpha}
      policy: ${overrides.sac_policy}
      target_update_interval: ${overrides.sac_target_update_interval}
      automatic_entropy_tuning: ${overrides.sac_automatic_entropy_tuning}
      target_entropy: ${overrides.sac_target_entropy}
      hidden_size: ${overrides.sac_hidden_size}
      device: ${device}
      lr: ${overrides.sac_lr}
dynamics_model:
  _target_: mbrl.models.GaussianMLP
  device: ${device}
  num_layers: 4
  in_size: ???
  out_size: ???
  ensemble_size: 7
  hid_size: 200
  deterministic: false
  propagation_method: random_model
  learn_logvar_bounds: false  # so far this works better
  activation_fn_cfg:
    _target_: torch.nn.SiLU
overrides:
  # @package _group_
  env: "dmcontrol___walker--run"
  term_fn: "no_termination"
  trial_length: 1000
  #save_dir: './exp_mpc_rollout/'
  #exp_name: 'cheetah_run_mbpo_0'

  num_steps: 400000
  epoch_length: 1000
  num_elites: 5
  patience: 5
  model_lr: 0.001
  model_wd: 0.00001
  model_batch_size: 256
  validation_ratio: 0.2
  freq_train_model: 250
  effective_model_rollouts_per_step: 400
  rollout_schedule: [ 20, 150, 1, 4 ]
  num_sac_updates_per_step: 10
  sac_updates_every_steps: 1
  num_epochs_to_retain_sac_buffer: 1

  sac_gamma: 0.99
  sac_tau: 0.005
  sac_alpha: 0.2
  sac_policy: "Gaussian"
  sac_target_update_interval: 1
  sac_automatic_entropy_tuning: true
  sac_target_entropy: -1
  sac_hidden_size: 512
  sac_lr: 0.0003
  sac_batch_size: 256
action_optimizer: cem

seed: 0
device: "cpu"
log_frequency_agent: 1000
save_video: false
debug_mode: false
is_mpc: 0

experiment: default

#root_dir: "./exp"
save_dir: './exp_mpc_rollout/'
exp_name: 'cheetah_run_mbpo_0'
hydra:
  run:
    dir: ${save_dir}/${exp_name}

  sweep:
    dir: ${root_dir}/${exp_name}